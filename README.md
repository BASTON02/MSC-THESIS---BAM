# MSC-THESIS---BAM---Time is Money: An Analysis of the Drivers of Cloud Performance When Hosting Credit Models

A repository containing all code and outputs related to the thesis project undertaken as part of the Masters degree programme, Msc Business Analytics and Management at Rotterdam School of Management. The project is focused on using synthetic data to assess the performance of cloud configurations on execution time. A smart sampling technique is used in combination with causal learning models, used to model the underlying, self injected relationships. The goal of the project is to evaluate 1. The drivers of cloud performance 2. Whether the methodology can detect the underlying relationships.

---

## Table of Contents

- [Overview](#overview)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [Data](#data)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## Overview
This project outlines the steps undertaken as part of this thesis research. Several steps took place during development, those being.

- Synthetic data generation
- Smart sampling using latin hypercube sampling
- Additional sampling using Bayesian Optimisation
- Descriptive analysis
- Data preparation
- Modeling using Hydranets
- Modeling using Two Layer FeedForward Neural Networks
- Analysis of results

## Project Structure

As for project structure, the most important files are:
- Live - MSc Business Analytics & Management - Thesis - Data analysis; MAIN python script that runs the full code back to front.
- CATES_across_configs; image of cates for hydranet
- CATES_across_configs_syn_tlffnn; image of cates for TLFFNN
- 3D_ConvexHull_Coverage; sample coverage, convex hull
- Sample_space_lhs_vs_bo; sample coverage BO vs LHS
- Coverage_sampling; Summary of sampled feature space per treatment variable

The project is unique as we generate our own data for the project, all done from inside of the script and it is never exported, implying no seperate files. Furthermore, all outputs can be consistently regenerated on command by running the script, thus, this script is the cornerstone to the project. In terms of other files, they represent the outputs of the models, indicating CATE estimates. Then again, these can easily be regenerated by the script itself. Additional images show the sampling process performance too such that we can analyse the feature space sampled.

## Installation
# Clone the repository
git clone [https://github.com/your-username/your-repo.git](https://github.com/BASTON02/MSC-THESIS---BAM)
cd MSC-THESIS---BAM

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate   # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

## Usage
- Open Live - MSc Business Analytics & Management - Thesis - Data analysis in folder. Run it in Juypter (ideally, using Python 3 kernel). Before executing anything, please search for "#Change by user" in the code. There are several places where outputs are exported too and the directories need to be adjusted such that they can be run on your PC. Without doing this, you will get an error. You can run the whole script from to back in batch and it will work without any intervention.
- Keep in mind that the script generates it own data and runs the training and predictions for one dataset only. The research project uses three datasets, one with no interaction between execution time and the treatment variables, one with some interaction and one with full, complex interactions. The script performs the full workflow for one dataset, not multiple. In order to do all three, you will need to uncomment out the following and recomment the following. In doing so, you select what dataset to use for that run.

#Uncomment for no interaction data
execution_results = configs.drop(columns=["execution_time_some", "execution_time_full"]).copy()
execution_results.rename(columns={"execution_time_none": "Execution_Time"}, inplace=True)

#Uncomment for some interaction data
# execution_results = configs.drop(columns=["execution_time_none", "execution_time_full"]).copy()
# execution_results.rename(columns={"execution_time_some": "Execution_Time"}, inplace=True)

#Uncomment for full interaction data
# execution_results = configs.drop(columns=["execution_time_none", "execution_time_some"]).copy()
# execution_results.rename(columns={"execution_time_full": "Execution_Time"}, inplace=True)

# mode can be "none", "some", or "full"
# df = generate_observations(df_live_bo_with_control, mode="full")
# df = generate_observations(df_live_bo_with_control, mode="some")
df = generate_observations(df_live_bo_with_control, mode="none")

There is no need to explicitly train the model or anything else of that matter, you can simply execute all code blocks when the changes above have been made and then the code can be run. Ensure not to overwrite the prevuious iterations results with the next one when running the next dataset.

## Data
Synthetic data was used for the whole project, no other data was used and thus there is no privacy issue. Furthermore, the data pre-processing is done automatically in the code, so no need to adjust it. The code handles the data generated and then processes it without any need for human involvement.

## Results
Key Takeaways and Achievements
- Causal Relationship Proven:
The study successfully demonstrated that execution time is causally influenced by three cloud configuration variables. This is a novel finding not previously shown in the literature.

- CATE Variation Confirmed:
It was also proven that Conditional Average Treatment Effects (CATEs) differ across configurations, indicating varied performance impacts.

- New Methodology Introduced:
A new performance testing method was established, combining causal inference with smart sampling â€” an approach not previously used in cloud performance testing.

Model Success:
- HydraNet consistently detected artificial relationships across all datasets.
- TLFFNN performed well except on the dataset with no embedded treatment effects but overfit the no interaction data.

## Contributing
This paper is part of my Msc thesis and is not open for external contributions. That said, please feel free to use the code and try to improve it. If it works, please reach out!

## License
This work is released under an academic research license for non-commercial use.

## Contact
Please contact me via BASTON02 as my github handle for any questions.











